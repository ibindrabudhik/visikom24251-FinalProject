{"cells":[{"cell_type":"markdown","metadata":{"id":"0AfPHfBpOqZD"},"source":["# Installation"]},{"cell_type":"markdown","metadata":{"id":"7QCijWcfOuM3"},"source":["Here we are going to install several libraries that we will use."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YzdnNfCLPx4o","outputId":"b125848a-848a-4ede-a183-10d540882a25","executionInfo":{"status":"ok","timestamp":1735551796938,"user_tz":-420,"elapsed":72282,"user":{"displayName":"Ida Bagus Indrabudhi Kusuma","userId":"15134787228146866023"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.9.1)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.6)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\n","Requirement already satisfied: gradio-client==1.5.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.2)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n","Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.4)\n","Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n","Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n","Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.9.0)\n","Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Install packages\n","!pip install gradio\n","!pip install -q git+https://github.com/huggingface/transformers.git\n","!pip install -q git+https://github.com/roboflow/supervision.git\n","!pip install -q accelerate\n","!pip install -q evaluate\n","!pip install -q roboflow\n","!pip install -q torchmetrics\n","!pip install -q \"albumentations>=1.4.5\"\n","!pip install -q datasets\n","!pip install -q safetensors # Model loading needs"]},{"cell_type":"markdown","metadata":{"id":"xleqo8q3PK-M"},"source":["# Import Libraries\n","Needed libraries are imported here"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"mKzaFoc3PM_I","executionInfo":{"status":"ok","timestamp":1735551796940,"user_tz":-420,"elapsed":523,"user":{"displayName":"Ida Bagus Indrabudhi Kusuma","userId":"15134787228146866023"}}},"outputs":[],"source":["from dataclasses import dataclass, replace\n","from functools import reduce\n","from io import BytesIO\n","import math\n","import os\n","from pprint import pprint\n","import tempfile\n","\n","from PIL import Image, ImageDraw, ImageFont\n","import numpy as np\n","import cv2\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch\n","from torch.utils.data import Dataset\n","import torchvision\n","from torchvision import transforms\n","\n","import roboflow\n","from roboflow import Roboflow\n","import supervision as sv\n","import albumentations as A\n","\n","import gradio as gr\n","import requests\n","\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","from torchmetrics.detection.iou import IntersectionOverUnion\n","import evaluate\n","#from datasets import load_metric\n","\n","from transformers import pipeline\n","from transformers import (\n","    AutoProcessor,\n","    AutoImageProcessor,\n","    AutoModel,\n","    AutoModelForObjectDetection,\n","    RTDetrForObjectDetection,\n","    RTDetrImageProcessor,\n","    TrainingArguments,\n","    Trainer\n",")\n","from huggingface_hub import hf_hub_download\n","\n","from google.colab import userdata\n","\n","from safetensors.torch import load_file"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"d02crSA6uTj2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735551796940,"user_tz":-420,"elapsed":427,"user":{"displayName":"Ida Bagus Indrabudhi Kusuma","userId":"15134787228146866023"}},"outputId":"7b01e966-e563-4a0d-e8dc-7b7c52686c92"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'Ambulance': 0, 'Firetruck': 1, 'Police': 2, 'Non-EV': 3}\n","{0: 'Ambulance', 1: 'Firetruck', 2: 'Police', 3: 'Non-EV'}\n"]}],"source":["#@title Utilities\n","PALETTE = {0: {\"color\": (255, 0, 0),\n","               \"name\": \"Ambulance\"},\n","           1: {\"color\": (0, 191, 0),\n","               \"name\": \"Firetruck\"},\n","           2: {\"color\": (0, 0, 255),\n","               \"name\": \"Police\"},\n","           3: {\"color\": (255, 0, 255),\n","               \"name\": \"Non-EV\"}}\n","label2id = {val[\"name\"]: id for (id, val) in PALETTE.items()}\n","id2label = {id: name for (name, id) in label2id.items()}\n","\n","print(label2id)\n","print(id2label)\n","\n","def unnormalize_bbox(img_h, img_w, bbox):\n","  x_min = bbox[0] - bbox[2]/2\n","  y_min = bbox[1] - bbox[3]/2\n","  x_max = bbox[0] + bbox[2]/2 # - x_min\n","  y_max = bbox[1] + bbox[3]/2 # - y_min\n","\n","  x_min *= img_w\n","  y_min *= img_h\n","  x_max *= img_w\n","  y_max *= img_h\n","  x_min, y_min, x_max, y_max = list(map(int, [x_min, y_min, x_max, y_max]))\n","\n","  return (x_min, y_min, x_max, y_max)\n","\n","def paint_bbox(\n","    image,\n","    annotations,\n","    normalize_labels=True,\n","    normalize_bbox=True,\n","  ):\n","  bboxes = annotations[\"boxes\"].tolist()\n","  class_id = annotations[\"labels\"].tolist()\n","  confidences = annotations[\"scores\"].tolist()\n","\n","  painted_img = image.copy() # Wutdehell\n","  for (bbox, label, confidence) in zip(bboxes, class_id, confidences):\n","    label = (label - 1) if normalize_labels else label\n","    if normalize_bbox:\n","      img_h, img_w = image.shape[0], image.shape[1] # H, W, C\n","      x_min, y_min, x_max, y_max = unnormalize_bbox(img_h, img_w, bbox)\n","      print([x_min, y_min, x_max, y_max])\n","\n","      \"\"\"\n","      x_min = #int(bbox[0] - bbox[2]/2) # Left\n","      y_min = #int(bbox[1] - bbox[3]/2) # Top\n","      x_max = #int(bbox[0] + bbox[2]/2)\n","      y_max = #int(bbox[1] + bbox[3]/2)\n","      \"\"\"\n","    else:\n","      x_min, y_min, x_max, y_max = list(map(int, bbox))\n","\n","    box_color = PALETTE[label][\"color\"]\n","    label_name = PALETTE[label][\"name\"]\n","\n","    if confidence != -1:\n","      label_name = f\"{label_name} ({confidence:.2f})\"\n","\n","    cv2.rectangle(painted_img,\n","                  (x_min, y_min),\n","                  (x_max, y_max),\n","                  color=box_color,\n","                  thickness=2)\n","    cv2.rectangle(painted_img,\n","                  (x_min, y_min),\n","                  (x_min + 5 + len(label_name)*10, y_min + 17),\n","                  color=box_color,\n","                  thickness=-1)\n","    cv2.putText(painted_img,\n","                label_name,\n","                (x_min + 2, y_min + 12),\n","                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n","                fontScale=0.5,\n","                color=(255, 255, 255),\n","                thickness=1)\n","  return painted_img"]},{"cell_type":"markdown","source":["# App & Evaluation"],"metadata":{"id":"NnJwAyQFNTi2"}},{"cell_type":"markdown","metadata":{"id":"SKUEhr5PPrF-"},"source":["## Detection Function"]},{"cell_type":"markdown","metadata":{"id":"XMRFMrWJQDdk"},"source":["detector output:\n","\n","[{'score': 0.997, 'label': 'bird', 'box': {'xmin': 69, 'ymin': 171, 'xmax': 396, 'ymax': 507}}, {'score': 0.999, 'label': 'bird', 'box': {'xmin': 398, 'ymin': 105, 'xmax': 767, 'ymax': 507}}]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"KIBYYYZfM2ir","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735551797474,"user_tz":-420,"elapsed":576,"user":{"displayName":"Ida Bagus Indrabudhi Kusuma","userId":"15134787228146866023"}},"outputId":"641a3ac5-720f-4884-959c-b2f1b16563e1"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at itsindrabudhik/finalProjectCV2425 were not used when initializing RTDetrForObjectDetection: ['class_labels_classifier.bias', 'class_labels_classifier.weight']\n","- This IS expected if you are initializing RTDetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RTDetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Device set to use cuda:0\n"]}],"source":["# Function to calculate Intersection over Union (IoU)\n","def calculate_iou(truth_bbx, pred_bbx):\n","    # Coordinates of the boxes: [xmin, ymin, xmax, ymax]\n","    x1, y1, x2, y2 = truth_bbx\n","    x1_p, y1_p, x2_p, y2_p = pred_bbx\n","\n","    # Calculate intersection\n","    ixmin = max(x1, x1_p)\n","    iymin = max(y1, y1_p)\n","    ixmax = min(x2, x2_p)\n","    iymax = min(y2, y2_p)\n","\n","    iw = max(0, ixmax - ixmin)\n","    ih = max(0, iymax - iymin)\n","\n","    intersection = iw * ih\n","    area1 = (x2 - x1) * (y2 - y1)\n","    area2 = (x2_p - x1_p) * (y2_p - y1_p)\n","    union = area1 + area2 - intersection\n","    iou = intersection / union if union != 0 else 0\n","    return iou\n","\n","# Example: emotion_classifier = pipeline(\"image-classification\", model=\"itsindrabudhik/emotion_classification\")\n","# (Load only once)\n","DETECTOR = pipeline(\"object-detection\", model=\"itsindrabudhik/finalProjectCV2425\") #later on, change this with out trained modell yesssss (the trained model should be uploaded to hugging face)\n","tensor_file = hf_hub_download(repo_id=\"itsindrabudhik/finalProjectCV2425\",\n","                               filename=\"model.safetensors\")\n","\n","\n","# Assign classification head weights since that pipeline seems to not handling it\n","# weights = load_file(tensor_file)\n","# DETECTOR.model.class_labels_classifier.weight.data = weights[\"class_labels_classifier.weight\"]\n","# DETECTOR.model.class_labels_classifier.bias.data = weights[\"class_labels_classifier.bias\"]\n","# del weights\n","\n","def detect_ev_nev(image, confidence_threshold=0.5, iou_threshold=0.5):\n","    # Run the detector pipeline on the image\n","    PALETTE = {0: {\"color\": (255, 0, 0),\n","                   \"name\": \"Ambulance\"},\n","               1: {\"color\": (0, 191, 0),\n","                   \"name\": \"Firetruck\"},\n","               2: {\"color\": (0, 0, 255),\n","                   \"name\": \"Police\"},\n","               3: {\"color\": (255, 0, 255),\n","                   \"name\": \"Non-EV\"}}\n","    results = DETECTOR(image)\n","\n","    # Open the image\n","    if isinstance(image, str):  # If the image is a URL or file path\n","        if image.startswith(\"http\"):\n","            response = requests.get(image)\n","            img = Image.open(BytesIO(response.content))\n","        else:\n","            img = Image.open(image)\n","    else:\n","        img = image\n","\n","    # Draw bounding boxes and labels on the image\n","    font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n","    font = ImageFont.truetype(font_path, size=32)\n","    draw = ImageDraw.Draw(img)\n","\n","    details = []  # Collect details for text output\n","    for result in results:\n","        score = result['score']\n","        label = result['label']\n","        box = result['box']\n","\n","        # Apply confidence threshold\n","        if score < confidence_threshold:\n","            continue\n","\n","        # Filter out low IoU detections\n","        keep = True\n","        for previous_result in results:\n","            if previous_result != result:\n","                prev_box = previous_result['box']\n","                iou = calculate_iou([box['xmin'], box['ymin'], box['xmax'], box['ymax']],\n","                                    [prev_box['xmin'], prev_box['ymin'], prev_box['xmax'], prev_box['ymax']])\n","                if iou > iou_threshold:\n","                    keep = False\n","                    break\n","\n","        label_color = PALETTE[label2id[label]][\"color\"]\n","        if keep:\n","            # Draw the bounding box and label\n","            xmin, ymin, xmax, ymax = box['xmin'], box['ymin'], box['xmax'], box['ymax']\n","            draw.rectangle([xmin, ymin, xmax, ymax], outline=label_color, width=3)\n","\n","            # Use a larger font size for text\n","            text = f\"{label} ({score:.2f})\"\n","\n","            # Calculate text bounding box\n","            text_bbox = draw.textbbox((xmin, ymin - 10), text, font=font)  # This gives (xmin, ymin, xmax, ymax)\n","            text_width = text_bbox[2] - text_bbox[0]  # width of the text box\n","            text_height = text_bbox[3] - text_bbox[1]  # height of the text box\n","\n","            # Draw the text on the image (position adjusted)\n","            draw.text((xmin, ymin - text_height - 5), text, fill=label_color, font=font)\n","\n","            # Add details to the list\n","            details.append({\n","                \"Label\": label,\n","                \"Confidence\": f\"{score:.2f}\",\n","                \"Bounding Box\": f\"({xmin}, {ymin}, {xmax}, {ymax})\"\n","            })\n","    details_text = \"\\n\".join([f\"Label: {d['Label']}, Confidence: {d['Confidence']}, Box: {d['Bounding Box']}\" for d in details])\n","    return img, details_text\n","\n","def detect_video(video, confidence_threshold=0.5, iou_threshold=0.5):\n","    PALETTE = {0: {\"color\": (255, 0, 0),\n","                   \"name\": \"Ambulance\"},\n","               1: {\"color\": (0, 191, 0),\n","                   \"name\": \"Firetruck\"},\n","               2: {\"color\": (0, 0, 255),\n","                   \"name\": \"Police\"},\n","               3: {\"color\": (255, 0, 255),\n","                   \"name\": \"Non-EV\"}}\n","\n","    video_capture = cv2.VideoCapture(video)\n","    fps = video_capture.get(cv2.CAP_PROP_FPS)\n","    frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","    temp_output = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\")\n","    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n","    out = cv2.VideoWriter(temp_output.name, fourcc, fps, (frame_width, frame_height))\n","\n","    details = []\n","    total_frames = 0\n","    detected_frames = 0\n","\n","    while True:\n","        ret, frame = video_capture.read()\n","        if not ret:\n","            break\n","\n","        total_frames += 1\n","        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","        annotated_image, frame_details = detect_ev_nev(image, confidence_threshold, iou_threshold)\n","\n","        # Count frames with detections\n","        if frame_details.strip():  # Non-empty details indicate detections\n","            detected_frames += 1\n","\n","        details.append(frame_details)\n","        annotated_frame = cv2.cvtColor(np.array(annotated_image), cv2.COLOR_RGB2BGR)\n","        out.write(annotated_frame)\n","\n","    video_capture.release()\n","    out.release()\n","\n","    details_text = \"\\n\".join(details)\n","    summary = f\"Total Frames: {total_frames}, Frames with Detections: {detected_frames}\\n\" + details_text\n","    return temp_output.name, summary\n","\n","def detect(file, confidence_threshold=0.5, iou_threshold=0.5):\n","    # Determine if input is an image or video\n","    file_ext = file.name.split(\".\")[-1].lower()\n","    if file_ext in [\"png\", \"jpg\", \"jpeg\"]:\n","        # Image processing\n","        annotated_image, details = detect_ev_nev(file, confidence_threshold, iou_threshold)\n","        return annotated_image, None, details\n","    elif file_ext in [\"mp4\", \"avi\", \"mov\"]:\n","        # Video processing\n","        processed_video, details = detect_video(file, confidence_threshold, iou_threshold)\n","        return None, processed_video, details\n","    else:\n","        raise ValueError(\"Unsupported file format. Please upload an image or video.\")\n"]},{"cell_type":"markdown","metadata":{"id":"mef0G0uNPPjQ"},"source":["# Interface"]},{"cell_type":"code","source":["\n","interface = gr.Interface(\n","    fn=detect,\n","    inputs=[\n","        gr.File(label=\"Upload Image or Video\", file_types=[\".png\", \".jpg\", \".jpeg\", \".mp4\", \".avi\", \".mov\"]),\n","        gr.Slider(0, 1, value=0.5, label=\"Confidence Threshold\"),\n","        gr.Slider(0, 1, value=0.5, label=\"IoU Threshold\"),\n","    ],\n","    outputs=[\n","\n","        gr.Image(label=\"Processed Image\"),\n","\n","        gr.Video(label=\"Generated Video\"),\n","        gr.Text(label=\"Detection Details\")\n","\n","    ],\n","    title=\"RT-DETR Object Detection for Images and Videos\",\n","    description=\"Upload an image or video to detect objects using the fine-tuned RT-DETR model. Results include the annotated image/video and detection details.\"\n",")\n","interface.launch(debug=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63U0K6tI2xUt","outputId":"8e331171-ea28-40d1-9555-57cd7d742f0a"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://2edf99d4c974007520.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["<div><iframe src=\"https://2edf99d4c974007520.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n","UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n","UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EIuxVEjbEhEO"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}